{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5b4db7-212d-4865-a76b-459a61aeda4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/venv_p5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "save_path = f\"/home/jovyan/hdfs-jmt-rungjoo-private/save_models/facet/baseline_bart\"\n",
    "# model_path = \"/home/jovyan/hdfs-jmt-rungjoo-private/huggingface_models/bart-base\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(save_path)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4994fddd-1fbc-4de1-9890-63841e51d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4b2994-5206-4079-8d82-092426455859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3245,   293,  2726,    23,   462, 26970,   343]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = \"100 best weight watchers recipes\"\n",
    "query_string = \"1016\"\n",
    "query_string = \"caesars atlantic city\"\n",
    "inputs = tokenizer(query_string, padding=True, truncation=True, max_length=tokenizer.model_max_length, return_tensors='pt', add_special_tokens=False)\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31bd8d3-e1de-494b-bf28-ca5c258e46f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caesars atlantic city'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a1c57b9-61fc-458a-ae10-f4a43f42806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = model.generate(inputs[\"input_ids\"])\n",
    "pred_facet_string = tokenizer.decode(token_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5099d81a-d948-4089-8143-98273211ed1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caesars parking atlantic city', 'caesars events atlatic city']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.strip() for x in pred_facet_string.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1ce45d1-f0c6-4c15-86e4-3a643f3a5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = {1:2}\n",
    "with open(\"result/baseline.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b499d37-cbec-415d-92bd-ea9ef9fd981b",
   "metadata": {},
   "source": [
    "## 비교모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1db434-d888-427c-b421-b57832e28182",
   "metadata": {},
   "source": [
    "### gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45fa2f0e-da47-4acf-b363-06555251ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "gpt3_facets = {}\n",
    "with jsonlines.open(\"Faspect/gpt3_facets.jsonl\") as f:\n",
    "    for line in f.iter():\n",
    "        query = line['query']\n",
    "        pred = line['facets']\n",
    "        gpt3_facets[query] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51526560-b8dc-43a8-b94b-f58e8e5b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result/baseline.json\", 'r', encoding='utf-8') as f:\n",
    "    baseline = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1204463a-c85a-444b-9ae7-799f3a398ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_facets_result = {}\n",
    "for ind, data in baseline.items():\n",
    "    query = data['query']\n",
    "    label = data['label']\n",
    "    pred = gpt3_facets[query]\n",
    "    \n",
    "    gpt3_facets_result[ind] = {}\n",
    "    gpt3_facets_result[ind]['query'] = query\n",
    "    gpt3_facets_result[ind]['pred'] = pred\n",
    "    gpt3_facets_result[ind]['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64b1d5f9-fb71-48c3-80db-cf05dd129dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result/gpt3_facets.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(gpt3_facets_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360983b-0b84-4932-a0a6-159fb1030cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ea85dd0-a4c8-4e4b-8282-0578fabf2bd6",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aad4c025-85c0-41ba-9dc5-ca2d292d0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result/baseline.json\", 'r', encoding='utf-8') as f:\n",
    "    result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a20384-d525-4982-92e9-becc03679421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from bert_score import score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def best_bleu_cand(groundtruth, candidate):\n",
    "#     assert len(groundtruth) >= len(candidate)\n",
    "    all_permutations = list(itertools.permutations(candidate))\n",
    "    max_bleu = 0.\n",
    "    best_cand = all_permutations[0]\n",
    "    for cand in all_permutations:\n",
    "        bleu = 0.\n",
    "        for i in range(min(len(groundtruth), len(cand))):\n",
    "            bleu += sentence_bleu([groundtruth[i]], cand[i]) / len(groundtruth)\n",
    "        if bleu > max_bleu:\n",
    "            max_bleu = bleu\n",
    "            best_cand = cand\n",
    "    return list(best_cand)\n",
    "\n",
    "\n",
    "def eval_bleu(groundtruth, cand):\n",
    "    # Calculates the SET BLEU metrics, for 1-gram, 2-gram, 3-gram and 4-gram overlaps\n",
    "    best_cand = best_bleu_cand(groundtruth, cand)\n",
    "    bleu = [0., 0., 0., 0.]\n",
    "    bleu_weights = [[1, 0, 0, 0], [0.5, 0.5, 0, 0], [0.33, 0.33, 0.33, 0], [0.25, 0.25, 0.25, 0.25]]\n",
    "    for j in range(4):\n",
    "        for i in range(min(len(groundtruth), len(best_cand))):\n",
    "            bleu[j] += sentence_bleu([groundtruth[i]], best_cand[i], weights=bleu_weights[j]) / len(groundtruth)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def bertscore(groundtruth, cand):\n",
    "    # Calculates the Set BERT-Score metrics for Precision, Recall & F1\n",
    "    best_cand = best_bleu_cand(groundtruth, cand)\n",
    "    (P, R, F), hashname = score(best_cand, groundtruth, lang=\"en\", return_hash=True, device=\"cuda:0\")\n",
    "    return P.mean().item(), R.mean().item(), F.mean().item()\n",
    "\n",
    "\n",
    "def exact_match(groundtruth, cand):\n",
    "    # Calculates the exact match Precision, Recall & F1\n",
    "    c = 0.\n",
    "    for x in cand:\n",
    "        if x != '' and x in groundtruth:\n",
    "            c += 1\n",
    "    p = c / (len([x for x in cand if x != ''])+1e-8)\n",
    "    r = c / (len([x for x in groundtruth if x != ''])+1e-8)\n",
    "    f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return [p, r, f1]\n",
    "\n",
    "\n",
    "def term_match(groundtruth, cand):\n",
    "    # Calculates the term overlap Precision, Recall & F1\n",
    "    gt_terms = set([])\n",
    "    for x in groundtruth:\n",
    "        if x == '':\n",
    "            continue\n",
    "        for t in x.strip().split():\n",
    "            gt_terms.add(t)\n",
    "    cand_terms = set([])\n",
    "    for x in cand:\n",
    "        if x == '':\n",
    "            continue\n",
    "        for t in x.strip().split():\n",
    "            cand_terms.add(t)\n",
    "\n",
    "    c = 0.\n",
    "    for x in cand_terms:\n",
    "        if x != '' and x in gt_terms:\n",
    "            c += 1\n",
    "    p = c / (len([x for x in cand_terms if x != ''])+1e-8)\n",
    "    r = c / (len([x for x in gt_terms if x != ''])+1e-8)\n",
    "    f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return [p, r, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96354a76-b186-4368-aed1-f00947ffeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0df07b1-d8c7-42ba-93e7-a0c3190f9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caesars atlantic city events', 'caesars atlantic city jobs', 'caesars atlantic city parking'] ['caesars parking atlantic city', 'caesars events atlatic city']\n"
     ]
    }
   ],
   "source": [
    "P_list, R_list, F_list = [], [], []\n",
    "for k, data in result.items():\n",
    "    pred_list = data['pred']\n",
    "    label_list = data['label']\n",
    "    break       \n",
    "print(label_list, pred_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6945bccd-0e00-4cbd-ab21-c5515c7d4911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'vista, ca',\n",
       " 'pred': ['weather', 'zip code', 'homes for sale vista', 'ca'],\n",
       " 'label': ['zip code', 'weather', 'population', 'homes for sale'],\n",
       " 'options_overall_label': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4abffbac-2b87-443c-ae09-ad607b3cf01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def best_bleu_cand(groundtruth, candidate):\n",
    "    copy_candidate = candidate[:]\n",
    "    best_cand = []\n",
    "    for facet in groundtruth:\n",
    "        max_bleu = 0\n",
    "        max_ind = 0\n",
    "        for i in range(len(copy_candidate)):\n",
    "            bleu = sentence_bleu([facet], copy_candidate[i])\n",
    "            if bleu > max_bleu:\n",
    "                max_bleu = bleu\n",
    "                max_ind = i\n",
    "        if len(copy_candidate) > 0:\n",
    "            best_cand.append(copy_candidate[max_ind])\n",
    "            copy_candidate.pop(max_ind)\n",
    "    return best_cand+copy_candidate\n",
    "    \n",
    "#     all_permutations = list(itertools.permutations(candidate))\n",
    "#     max_bleu = 0.\n",
    "#     best_cand = all_permutations[0]\n",
    "#     for cand in all_permutations:\n",
    "#         bleu = 0.\n",
    "#         for i in range(min(len(groundtruth), len(cand))):\n",
    "#             bleu += sentence_bleu([groundtruth[i]], cand[i]) / len(groundtruth)\n",
    "#         if bleu > max_bleu:\n",
    "#             max_bleu = bleu\n",
    "#             best_cand = cand\n",
    "#     return list(best_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c02cfb7f-cee6-437e-894f-fb64e7632d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caesars atlantic city events', 'caesars atlantic city jobs', 'caesars atlantic city parking'] ['caesars parking atlantic city', 'caesars events atlatic city']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['caesars events atlatic city', 'caesars parking atlantic city']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(label_list, pred_list)\n",
    "best_bleu_cand(label_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d74253bf-a935-4ea1-9590-276147e2fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caesars atlantic city events', 'caesars atlantic city jobs', 'caesars atlantic city parking'] ['caesars parking atlantic city', 'caesars events atlatic city', 'caesars', 'caesars good', 'caesars', 'caesars good']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['caesars events atlatic city',\n",
       " 'caesars parking atlantic city',\n",
       " 'caesars good',\n",
       " 'caesars',\n",
       " 'caesars',\n",
       " 'caesars good']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list.append(\"caesars\")\n",
    "pred_list.append(\"caesars good\")\n",
    "print(label_list, pred_list)\n",
    "best_bleu_cand(label_list, pred_list[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10680b00-76f4-4661-ba60-d44b0e71691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc831caf-54c0-49d0-ae9e-7770d544bea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.958801805973053, 0.9332197904586792],\n",
       " [0.9588017463684082, 0.9316922426223755],\n",
       " [0.958801805973053, 0.9324553608894348])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1 = results['precision'], results['recall'], results['f1']\n",
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81417cd0-954e-4075-8e1b-dd2f6f22cc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4ae45-55cb-4f0e-a996-b1de94828137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de2619-f3b2-418d-a7ec-13b3028ea1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = [\"for sale\", \"used cars\", \"electric\", \"cheap\"]\n",
    "cand = [\"afforable cars\", \"cars for sale\", \"used\", \"electric\"]\n",
    "cand = best_bleu_cand(groundtruth, cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432c7a9f-d9a0-40bf-baf4-65293369f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term overlap metrics: P=0.8333333319444445,R=0.8333333319444445,F1=0.8333333319444444\n",
      "Exact match metrics: P=0.249999999375,R=0.249999999375,F1=0.249999999375\n"
     ]
    }
   ],
   "source": [
    "term_overlap_metrics = term_match(groundtruth, cand)\n",
    "print(\"Term overlap metrics: P={},R={},F1={}\".format(term_overlap_metrics[0],\n",
    "                                                     term_overlap_metrics[1],\n",
    "                                                     term_overlap_metrics[2]))\n",
    "\n",
    "exact_match_metrics = exact_match(groundtruth, cand)\n",
    "print(\"Exact match metrics: P={},R={},F1={}\".format(exact_match_metrics[0],\n",
    "                                                    exact_match_metrics[1],\n",
    "                                                    exact_match_metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b78e82-aa7b-42d6-aad5-8375e3b6b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/venv_p5/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/venv_p5/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/venv_p5/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.52904378163263,\n",
       " 0.47141237159386706,\n",
       " 0.4675886902868809,\n",
       " 0.46146832211337435]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bleu(groundtruth, cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9146c3c3-6172-49de-b3eb-b3a44f4ac5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT score metrics: P=0.9065482020378113,R=0.9197155237197876,F1=0.9125113487243652\n"
     ]
    }
   ],
   "source": [
    "bert_score_metrics = bertscore(groundtruth, cand)\n",
    "print(\"BERT score metrics: P={},R={},F1={}\".format(bert_score_metrics[0],\n",
    "                                                   bert_score_metrics[1],\n",
    "                                                   bert_score_metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45af33a5-6d1b-4d41-af32-3f54e7a5c2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n",
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  q\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home1/irteam/nltk_data'\n    - '/home1/jovyan/venv_main/nltk_data'\n    - '/home1/jovyan/venv_main/share/nltk_data'\n    - '/home1/jovyan/venv_main/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m      7\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAt eight o\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclock on Thursday morning\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    Arthur didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt feel very good.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[0;32m/home1/jovyan/venv_main/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/home1/jovyan/venv_main/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/home1/jovyan/venv_main/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/home1/jovyan/venv_main/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/home1/jovyan/venv_main/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home1/irteam/nltk_data'\n    - '/home1/jovyan/venv_main/nltk_data'\n    - '/home1/jovyan/venv_main/share/nltk_data'\n    - '/home1/jovyan/venv_main/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore_func = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09dd85-5cef-463a-b167-f8b4d8740406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_main",
   "language": "python",
   "name": "venv_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
